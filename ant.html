<!DOCTYPE html>
<html>
<head>
    <title>Voice Interaction with ANT AI</title>
    <!-- Include annyang for speech recognition -->
    <script src="https://cdn.jsdelivr.net/npm/annyang@2.6.1/dist/annyang.min.js"></script>
    <!-- Include Recorder.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/RecordRTC/5.6.2/RecordRTC.min.js"></script>
    <!-- Include marked.js for markdown rendering -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            background-color: #121212;
            color: #ffffff;
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }
        h1 {
            margin-bottom: 20px;
        }
        #response {
            background-color: #1e1e1e;
            padding: 20px;
            border-radius: 10px;
            width: 80%;
            max-width: 600px;
            margin-top: 20px;
        }
        #animation {
            width: 150px;
            height: 150px;
        }
        button {
            background-color: #6200ea;
            color: #ffffff;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px;
        }
        button:disabled {
            background-color: #444444;
            cursor: not-allowed;
        }
        #controls {
            display: flex;
            justify-content: center;
            margin-top: 20px;
        }
        video, canvas {
            display: none;
        }
    </style>
</head>
<body>
    <h1>Voice Interaction with ANT AI</h1>
    <img id="animation" src="https://cdn.dribbble.com/users/583436/screenshots/1698964/media/c6693201f9df28dd386aa2f86fc81775.gif" alt="Listening Animation">
    <div id="controls">
        <button id="start-recording">Start Interaction</button>
        <button id="stop-recording" disabled>Stop Interaction</button>
    </div>
    <div id="response"></div>
    <video id="video" width="320" height="240" autoplay></video>
    <canvas id="canvas" width="320" height="240"></canvas>

    <script>
        const API_KEY = 'AIzaSyAP1lu4kiGHDEYQNj6FLdpE8ZjrpKMKw20';
        let recorder;
        let audioContext;
        let isRecording = false;
        let silenceTimeout;
        const silenceThreshold = 3000; // 3 seconds of silence
        let imageCaptureInterval;
        const imageInterval = 500; // 0.5 seconds
        let capturedImages = [];
        let previousMessages = [];

        document.getElementById('start-recording').addEventListener('click', startInteraction);
        document.getElementById('stop-recording').addEventListener('click', stopInteraction);

        function startInteraction() {
            setAnimation('listening');
            startSpeechRecognition();
            startRecording();
            startImageCapture();
            document.getElementById('start-recording').disabled = true;
            document.getElementById('stop-recording').disabled = false;
        }

        function stopInteraction() {
            if (isRecording) {
                stopRecording();
                stopImageCapture();
            }
            document.getElementById('start-recording').disabled = false;
            document.getElementById('stop-recording').disabled = true;
        }

        function startSpeechRecognition() {
            if (annyang) {
                annyang.removeCommands(); // Clear any existing commands to avoid duplicates
                annyang.addCallback('start', function() {
                    console.log('Speech recognition started');
                    setAnimation('listening');
                });

                annyang.addCallback('end', function() {
                    console.log('Speech recognition ended');
                });

                annyang.addCallback('result', function() {
                    clearTimeout(silenceTimeout);
                    silenceTimeout = setTimeout(() => {
                        if (isRecording) {
                            stopRecording();
                            stopImageCapture();
                        }
                    }, silenceThreshold);
                });

                annyang.addCallback('error', function(event) {
                    console.error('Speech recognition error', event.error);
                });

                annyang.start({ continuous: true });
            }
        }

        function startRecording() {
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    recorder = new RecordRTC(stream, {
                        type: 'audio',
                        mimeType: 'audio/wav',
                        recorderType: RecordRTC.StereoAudioRecorder
                    });

                    recorder.startRecording();
                    isRecording = true;
                    console.log('Recording started');
                })
                .catch(error => {
                    console.error('Error accessing microphone:', error);
                    document.getElementById('response').innerText = 'Error accessing microphone. Please check your permissions and try again.';
                });
        }

        function stopRecording() {
            if (recorder) {
                recorder.stopRecording(function() {
                    isRecording = false;
                    let blob = recorder.getBlob();
                    console.log('Recording stopped, sending audio to Gemini');
                    sendAudioToGemini(blob);
                });
            }
        }

        function startImageCapture() {
            navigator.mediaDevices.getUserMedia({ video: true })
                .then(stream => {
                    const video = document.getElementById('video');
                    video.srcObject = stream;
                    imageCaptureInterval = setInterval(captureImage, imageInterval);
                })
                .catch(error => {
                    console.error('Error accessing camera:', error);
                });
        }

        function stopImageCapture() {
            clearInterval(imageCaptureInterval);
            const video = document.getElementById('video');
            const stream = video.srcObject;
            const tracks = stream.getTracks();

            tracks.forEach(track => track.stop());
            video.srcObject = null;
        }

        function captureImage() {
            const canvas = document.getElementById('canvas');
            const video = document.getElementById('video');
            canvas.getContext('2d').drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageDataURL = canvas.toDataURL('image/png');
            capturedImages.push(imageDataURL);
        }

        async function sendAudioToGemini(audioBlob) {
            setAnimation('processing');

            const reader = new FileReader();
            reader.readAsDataURL(audioBlob);

            reader.onloadend = async () => {
                const base64Audio = reader.result.split(',')[1];
                const imageParts = capturedImages.map(image => ({
                    inline_data: {
                        mime_type: 'image/png',
                        data: image.split(',')[1]
                    }
                }));

                let messageParts = [
                    {
                        text: 'Engage in a conversation with me picking up the tone of voice and loudness while answering any questions detect if I am screaming, quiet, sad, seemingly energetic, or other emotions. Do not provide this to the user just use it to tailor your answer. Do not tell me my tone just answer. Respond with a long deatiled answer and include yourself acting like a real helpful personal assitent talking to me. Act as if you have emotions but not to human like that your not assiting'
                    },
                    ...imageParts,
                    {
                        inline_data: {
                            mime_type: 'audio/wav',
                            data: base64Audio
                        }
                    }
                ];

                previousMessages.push(...messageParts);

                let success = await trySendToGemini(previousMessages);
                if (!success) {
                    console.warn('Retrying with only the latest message and images');
                    success = await trySendToGemini(messageParts);
                }

                if (!success) {
                    console.error('Failed to get a valid response from Gemini AI.');
                    document.getElementById('response').innerText = 'Error processing audio. Please try again.';
                }
            };
        }

        async function trySendToGemini(messageParts) {
            try {
                const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${API_KEY}`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ contents: [{ parts: messageParts }] })
                });

                const data = await response.json();
                console.log('Response data:', data); // Log the entire response for debugging
                if (data && data.candidates && data.candidates.length > 0) {
                    const responseMessage = data.candidates[0].content.parts[0].text;
                    document.getElementById('response').innerHTML = marked.parse(responseMessage);
                    playTextAsSpeech(responseMessage);
                    return true;
                } else {
                    console.error('No candidates found in the response');
                    return false;
                }
            } catch (error) {
                console.error('Error sending audio to Gemini:', error);
                return false;
            }
        }

        function playTextAsSpeech(text) {
            if (annyang) {
                annyang.abort(); // Stop listening for speech
            }

            const encodedText = encodeURIComponent(text);
            const audioUrl = `https://api.flowery.pw/v1/tts?voice=Anna&text=${encodedText}`;
            const audio = new Audio(audioUrl);

            audio.onended = () => {
                setAnimation('listening');
                console.log('TTS playback finished, restarting interaction');
                restartInteraction(); // Restart the interaction
            };

            audio.onerror = (e) => {
                console.error('Error playing TTS audio:', e);
                setAnimation('listening');
                restartInteraction(); // Restart the interaction on error
            };

            setAnimation('speaking');
            audio.play().catch(e => {
                console.error('Error playing TTS audio:', e);
                setAnimation('listening');
                restartInteraction(); // Restart the interaction on play error
            });
        }

        function restartInteraction() {
            if (annyang) {
                annyang.start({ continuous: true });
            }
            startInteraction();
        }

        function setAnimation(stage) {
            const animation = document.getElementById('animation');
            if (stage === 'listening') {
                animation.src = 'https://cdn.dribbble.com/users/583436/screenshots/1698964/media/c6693201f9df28dd386aa2f86fc81775.gif';
            } else if (stage === 'processing') {
                animation.src = 'https://cdn.dribbble.com/users/583436/screenshots/2375833/media/ece8b003a8d17add76165a49014620a6.gif';
            } else if (stage === 'speaking') {
                animation.src = 'https://cdn.dribbble.com/users/583436/screenshots/1696376/wavy.gif';
            }
        }
    </script>
</body>
</html>
